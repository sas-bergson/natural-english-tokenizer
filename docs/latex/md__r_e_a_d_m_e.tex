In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters into a sequence of lexical tokens.\hypertarget{md__r_e_a_d_m_e_autotoc_md1}{}\doxysection{Team Members}\label{md__r_e_a_d_m_e_autotoc_md1}
sandrine Awah Nwabufo.\+T.\+Emmanuel Junior 