# natural-english-tokenizer
In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters into a sequence of lexical tokens.

# Team Members
sandrine Awah 
Nwabufo.T.Emmanuel Junior 

