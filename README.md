# natural-english-tokenizer
In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters into a sequence of lexical tokens.

# Team Members
sandrine Awah 
Nwabufo.T.Emmanuel Junior 

# Class sentence
 it contains several method
  
    f = open('./assets/sample.txt', 'r') 
    sample = f.read()
    f.close() 
    
 # def sentence_check(text): 
 # This method simple print out a correct sentence following the pattern
 pattern = re.compile('[A-Z][^\.!?]*[!?\.]')
 if re.match(pattern, text):
 text
 print(text)
 return text
 else:
 print("Invalid")
        
